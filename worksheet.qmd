---
title: "Using simulations for power analyses and sample size planning<br> Worksheet"
author: "Matteo Lisi"
format:
  html:
    toc: true
    code-tools: true     # run/copy buttons
    code-fold: show
execute:
  echo: true
  warning: false
  message: false
---

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
library(rstan)
library(lme4)
```

## Activity 1 — Experience sampling study

In this activity, we simulate data from an experience sampling (or daily diary) study in which the same participants are measured repeatedly over several days. We then use a multilevel (mixed-effects) model to analyse the data and estimate statistical power for key effects of interest. We can set up our simulations to account for missing observations, or autocorrelation in the predictor.

### Between-person and within-person effects

A central feature of experience sampling data is that observations are **nested within people**. This means that a predictor can vary in two distinct ways:

- **Between people**: some participants tend to score higher than others *on average*.
- **Within a person**: a given participant fluctuates around their own average from day to day.

These two sources of variation often have different psychological meanings and should be analysed separately.

To do this, we decompose the predictor $X$ into two components:

- **Between-person component** (`x_bar`):  
  each participant’s average level of $X$ across days.
- **Within-person component** (`x_cwc`):  
  the day-to-day deviation from that participant’s own mean  
  (often called *cluster-mean centering* or *person-mean centering*).

In the multilevel model, this leads to two separate effects:

- The **between-person effect** (`beta_b`):  
  Do people who are higher on average on $X$ also tend to have higher values of the outcome $Y$?
- The **within-person effect** (`beta_w`):  
  On days when a person is higher than *their own* average on $X$, do they also tend to show higher values of $Y$?

Importantly, these effects need not be the same in size, or even in direction. Treating them separately avoids conflating *differences between people* with *changes within a person over time*.

In this simulation, both components are standardised separately. This means that effect sizes such as `beta_w = 0.20` should be interpreted as:
> a 0.20 SD change in the outcome for a 1 SD change in the corresponding component of the predictor.

This is convenient for setting plausible effect sizes based on common “small / medium / large” conventions.

---

### Temporal dependence and the AR(1) process

In many experience sampling studies, measurements taken close together in time are more similar than measurements taken further apart. For example, how someone feels today is likely to be related to how they felt yesterday. This is known as **temporal autocorrelation**.

In this activity, we model temporal dependence in the *within-person deviations of the predictor* using an **AR(1) process** (autoregressive process of order 1).

#### The AR(1) idea

An AR(1) process assumes that each value depends partly on the previous value plus some new random fluctuation:

$$ \text{out}_t = r \cdot \text{out}_{t-1} + \text{innovation}_t $$

where:
- $r$ is the autocorrelation parameter (between −1 and 1),
- $\text{innovation}_t$ is new random noise at time $t$.

If $r = 0$, observations are independent over time.  
If $r > 0$, consecutive observations tend to be similar ("sticky" over time).

#### Why the variance matters

We want the simulated time series to have a **stable (stationary) variance** over time. For a stationary AR(1) process, the variance of the series satisfies:

$$ \mathrm{Var}(\text{out}) = r^2 \, \mathrm{Var}(\text{out}) + \mathrm{Var}(\text{innovation}) $$

This equation reflects the fact that:
- part of the variance comes from carrying over the previous value ($r^2 \, \mathrm{Var}(\text{out})$),
- part comes from new random input ($\mathrm{Var}(\text{innovation})$).

Solving for the variance of the process gives:

$$ \mathrm{Var}(\text{out}) = \frac{\mathrm{Var}(\text{innovation})}{1 - r^2} $$

This shows that stronger autocorrelation (larger $r$) increases the overall variability of the time series for a given amount of noise.

#### Choosing the innovation variance

In the simulation, we want the AR(1) process to have a **target standard deviation** $s$ (e.g., the within-person SD of the predictor). To achieve this, we set the innovation variance so that:

$$ \mathrm{Var}(\text{out}) = s^2 $$

Substituting into the previous equation:

$$ s^2 = \frac{\mathrm{Var}(\text{innovation})}{1 - r^2} $$

which implies:

$$ \mathrm{SD}(\text{innovation}) = s \sqrt{1 - r^2} $$

This ensures that, regardless of the value of $r$, the simulated within-person deviations have the desired overall variability.

Note that here we include autocorrelation in the predictor but assume that residual errors in the outcome are independent. This keeps the fitted multilevel model correctly specified, while still allowing temporal structure in the data-generating process. It is possible to relax this assumption, but then the model (as it is formulated now, using the `lme4` library) would be miss-specified. This can be OK (perhaps we want to check the effect of model misspecification on power or on false-positive rate). To fit a model that explicitly allows for residual autocorrelation we need a different package (see `?corAR1` in the `nlme` package).

### Code: data simulation

For this example we need these packages:

```{r}
library(lme4)
library(lmerTest)
library(tidyverse)
library(MASS)   # for mvrnorm()
```

We first define a custom function for the AR(1) process, to simulate a vector os size `n` with a set variance `s` and autocorrelation `r`.

```{r}
r_ar1 <- function(n, s, r){
  innov_sd <- s * sqrt(1 - r^2)
  innov <- rnorm(n, mean = 0, sd = innov_sd)
  
  out <- numeric(n)
  out[1] <- rnorm(1, mean = 0, sd = s)
  for (t in 2:n) {
    out[t] <- r * out[t - 1] + innov[t]
  }
  out
}
```


The actual data-simulation function:

```{r}
sim_diary <- function(
    n_child = 50,
    n_days  = 7,
    
    # Predictor X structure
    sd_between_x = 0.6,  # SD of each child's mean X across children (between-person variability)
    sd_within_x  = 0.8,  # SD of day-to-day deviations within a child (within-person variability)
    rho_x_ar1    = 0.2,  # AR(1) correlation in within-person deviations of X across consecutive days
    
    # Outcome Y structure
    beta0   = 0,         # intercept
    beta_w  = 0.20,      # within-person effect of x_cwc on Y  (in SD units of x_cwc)
    beta_b  = 0.20,      # between-person effect of x_bar on Y (in SD units of x_bar)
    tau0    = 0.60,      # SD of random intercepts
    tau1    = 0.15,      # SD of random slopes for x_cwc
    rho_u   = 0.0,       # correlation between intercept and slope random effects
    sigma_e = 0.80,      # residual SD (day-level noise), assumed independent over days
    
    weekday_fx = FALSE,  # include weekday fixed effects in the fitted model?
    
    # Missingness (day-level MCAR)
    mcar_rate = 0.05,    # probability any given day is missing (independent of everything)
    
    seed = NULL
){
  if (!is.null(seed)) set.seed(seed)
  
  # --- Random effects covariance matrix for (u0, u1) ---
  Sigma_u <- matrix(
    c(tau0^2,              rho_u * tau0 * tau1,
      rho_u * tau0 * tau1,        tau1^2),
    nrow = 2, byrow = TRUE
  )
  
  # --- Balanced panel: each child measured each day ---
  df <- expand.grid(id = 1:n_child, day = 1:n_days) %>%
    as_tibble() %>%
    arrange(id, day) %>%
    mutate(
      weekday = factor((day - 1) %% 7,
                       labels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun"))
    )
  
  # --- Person-level mean of X (between-person differences) ---
  x_bar <- rnorm(n_child, mean = 0, sd = sd_between_x)
  df <- df %>%
    left_join(tibble(id = 1:n_child, x_bar = x_bar), by = "id")
  
  # --- Within-person daily deviations of X with AR(1) structure ---
  df <- df %>%
    group_by(id) %>%
    mutate(x_within = r_ar1(n_days, sd_within_x, rho_x_ar1)) %>%
    ungroup() %>%
    mutate(
      x     = x_bar + x_within,
      x_cwc = x - x_bar
    )
  
  # --- Standardise predictors separately ---
  # IMPORTANT: beta_w and beta_b are interpreted in SD units of each component (x_cwc and x_bar).
  df <- df %>%
    mutate(
      x     = as.numeric(scale(x)),
      x_bar = as.numeric(scale(x_bar)),
      x_cwc = as.numeric(scale(x_cwc))
    )
  
  # --- Random effects per child (random intercept + random slope for x_cwc) ---
  RE <- MASS::mvrnorm(n_child, mu = c(0, 0), Sigma = Sigma_u)
  RE <- tibble(id = 1:n_child, u0 = RE[,1], u1 = RE[,2])
  df <- df %>% left_join(RE, by = "id")
  
  # --- Day-level residuals e (independent across days) ---
  # This keeps the fitted lmer() model correctly specified w.r.t. residual correlation.
  df <- df %>%
    mutate(e = rnorm(nrow(df), mean = 0, sd = sigma_e))
  
  # --- True weekday effect (set to zero here; weekday_fx just changes the fitted model) ---
  df <- df %>% mutate(weekday_eff = 0)
  
  # --- Outcome generation ---
  df <- df %>%
    mutate(
      y = beta0 + u0 +
        (beta_w + u1) * x_cwc +
        beta_b * x_bar +
        weekday_eff +
        e
    )
  
  # Optional: standardise outcome
  df <- df %>% mutate(y = as.numeric(scale(y)))
  
  # --- Missingness: day-level MCAR only ---
  if (mcar_rate > 0) {
    keep <- runif(nrow(df)) >= mcar_rate
    df <- df[keep, ]
  }
  
  df
}
```


### Code: fit model and extract statistics

It is convenient to wrap the code for fitting the model and extracting the statistic of interest in a custom function.

```{r}
fit_once <- function(df, weekday_fx = TRUE) {
  
  if (weekday_fx) {
    f <- y ~ x_cwc + x_bar + weekday + (1 + x_cwc | id)
  } else {
    f <- y ~ x_cwc + x_bar + (1 + x_cwc | id)
  }
  
  m <- suppressMessages(lmer(f, data = df, REML = FALSE))
  
  broom.mixed::tidy(m, effects = "fixed", conf.int = TRUE) %>%
    filter(term %in% c("x_cwc", "x_bar")) %>%
    transmute(
      term,
      est = estimate,
      lwr = conf.low,
      upr = conf.high,
      p   = p.value
    )
}
```

### Code: power simulation

We can finally put it all together in a function that iteratively simulate a number of datasets, fit the model on each simulated dataset, and then compute the power as the fraction of significant simulations

```{r}
power_grid <- function( n_sims = 100, weekday_fx = FALSE, ...) {
  all_sims <- vector("list", n_sims)
  
  for (s in 1:n_sims) {
    df  <- sim_diary(seed = s, weekday_fx = weekday_fx, ...)
    est <- fit_once(df, weekday_fx = weekday_fx)
    est$sim <- s
    all_sims[[s]] <- est
  }
  
  res <- bind_rows(all_sims)
  
  res %>%
    group_by(term) %>%
    summarise(
      power_0.05 = mean(p < 0.05, na.rm = TRUE),
      median_est = median(est, na.rm = TRUE),
      mean_est   = mean(est, na.rm = TRUE),
      SE_est     = sd(est, na.rm = TRUE),
      q25        = quantile(est, 0.25, na.rm = TRUE),
      q75        = quantile(est, 0.75, na.rm = TRUE),
      .groups    = "drop"
    )
}
```

Example usage:

```{r, eval=FALSE}

quick_A <- power_grid(
  n_sims = 200,
  n_child = 50,
  n_days = 7,
  beta_w = 0.20,
  beta_b = 0.20,
  tau0 = 0.60,
  tau1 = 0.15,
  sigma_e = 0.80,
  rho_x_ar1 = 0.2,
  mcar_rate = 0.05
)

quick_A

```

