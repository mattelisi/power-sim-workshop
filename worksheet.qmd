---
title: "Using simulations for power analyses and sample size planning<br> Worksheet"
author: "Matteo Lisi"
format:
  html:
    toc: true
    code-tools: true     # run/copy buttons
    code-fold: show
execute:
  echo: true
  warning: false
  message: false
---

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
library(rstan)
library(lme4)
```

## Activity 1 — Planning a delayed discounting study

In this activity, we simulate data from a **delayed discounting task**, a commonly used paradigm to study impulsive decision-making. On each trial, participants choose between:

- a **smaller reward available immediately**, and  
- a **larger reward available after a delay**.

Across trials, the amounts of the immediate and delayed rewards are systematically varied. This allows us to estimate a *psychometric choice function* describing how the probability of choosing the delayed reward changes as the difference between delayed and immediate rewards increases.

Two key features of behaviour in this task are:

- **Bias (or PSE, point of subjective equality):**  
  the reward difference at which a participant is equally likely to choose the immediate or delayed option. Lower PSE values indicate greater impulsivity (a stronger preference for immediate rewards).
- **Slope:**  
  how sharply choices switch from preferring the immediate to the delayed option as the reward difference increases. Shallower slopes indicate noisier or less consistent preferences.

**Why might we want to simulate this task?** Delayed discounting tasks are widely used in clinical and cognitive neuroscience to study impulsivity and reward processing. For example, one possible application is in Parkinson’s disease, where dopaminergic medication (such as L-DOPA or dopamine agonists) has been linked to changes in impulsivity and reward sensitivity. Some studies suggest that dopaminergic treatment can increase impulsive choice in certain patients, although effects can vary across individuals.

In this example, we simulate two groups (e.g. patients *on* vs *off* medication, or patients vs controls), assuming a small group difference in the underlying decision parameters. Rather than analysing raw choice proportions, we model behaviour at the level of latent psychometric parameters and use simulation to explore how sample size, effect size, and individual variability affect statistical power.

### Task design

We can fix the set of choices offered in the task; I am using in particular a design that has been used in prior studies, which aims to provide a targeted sampling of possible choice pairs (combinations of immediate and delayed rewards) in a way that should encompass a broad range of individual differences in discounting.

```{r}
immediate <- c(
  rep(10, 15),
  13:27,
  rep(20, 15),
  seq(16, 44, length.out = 15)
)

delayed <- c(
  11:25,
  rep(30, 15),
  24:38,
  rep(45, 15)
)

design_dd <- data.frame(
  trial = 1:length(immediate),
  immediate = immediate,
  delayed = delayed
)

```

We can visualise this with a scatterplot

```{r}
#| fig.align: 'center'
#| fig.height: 4
#| fig.width: 4.5

plot(design_dd$immediate, design_dd$delayed, pch=19, col="blue",
     xlab="immediate reward [£]", ylab="delayed reward [£]")
abline(a=0,b=1, lty=2) # identity line

```


### Code: data simulation


Here is the function used to simulate the data. A few things to notice here:

- **Random variation operates at multiple levels.**  
  We sample random quantities at two distinct levels:  
  (i) *participant-level parameters* that capture individual differences (e.g. bias and slope), drawn from a **multivariate normal distribution**, and  
  (ii) *trial-level choices*, generated from a **Bernoulli (binomial) distribution** given those parameters.  
  This hierarchical structure—multiple sources of variability at different levels—is precisely why models for these data are referred to as *multilevel* models. At the participant level, we allow the bias and variability parameters to be correlated, reflecting realistic individual differences in decision-making.

- **Defining a “small” effect size in a GLMM.**  
  We define the effect size in terms of the population-level SD across participant, so we multiply the Cohen's $d$ by the standard deviation $\Delta_{\text{PSE}} = d \times \text{SD}_{\text{PSE}}$. Note that as we define effect size in SD units of latent PSE (between-person SD) does not include variability due to "measurement" error (which depends on task design, number of trials, etc.)

  <!-- We specify a small effect size as Cohen’s $d = 0.2$. Strictly speaking, Cohen’s $d$ is defined for linear models, and there is no exact analogue for non-linear models such as logistic or probit GLMs. A commonly used and pragmatic approximation is to convert a standardised effect size into the log-odds scale by multiplying by the standard deviation of the standard logistic distribution: $$\Delta_{\text{log-odds}} \;\approx\; \text{Cohen’s } d \times \frac{\pi}{\sqrt{3}}.$$ -->
  <!-- Importantly, this approximation transform the effect on the log-odds scale (the linear predictor of the GLM), whereas our measure of interest truly is defined in terms of the PSE (Point of Subjective Equality: the difference value for which a participant is indifferent to choosing an immediate or delayed reward). We can further convert this as follow: $$\Delta_{\text{PSE}}= - \frac{\Delta_{\text{log-odds}}}{\text{slope}},$$where slope is the parameter that determine the steepness of the choice function.  -->
  <!-- Because slope varies across participants, again there isn’t a single exact conversion. Using `mu_slope` (the baseline group mean slope) is a reasonable and common approximation.  -->


```{r}
simulate_dd <- function(
    n_per_group = 30,
    groups = c("CL", "PD"),
    # ---- population-level parameters (baseline group) ----
    mu_pse   = 12.5,   # mean point of subjective equality (bias)
    mu_slope = 0.3,    # mean slope of psychometric function
    sd_pse   = 2.0,    # between-subject SD of PSE
    sd_slope = 0.15,   # between-subject SD of slope
    rho      = 0,      # correlation between PSE and slope
    # ---- group effect (small effect, approx d = 0.2) ----
    d_effect = 0.2,    # Cohen's d
    
    seed = NULL
){
  
  if (!is.null(seed)) set.seed(seed)
  
  # we scale tghe effect size according to the SD across people
  delta <- d_effect * sd_pse
  
  # ---- covariance matrix for (PSE, slope) ----
  Sigma <- matrix(
    c(sd_pse^2,
      rho * sd_pse * sd_slope,
      rho * sd_pse * sd_slope,
      sd_slope^2),
    nrow = 2, byrow = TRUE
  )
  
  dat <- list()
  row_i <- 1
  
  for (g in groups) {
    
    # group-specific mean shift
    mu_shift_pse   <- ifelse(g != groups[1], delta, 0)
    
    mu_vec <- c(mu_pse + mu_shift_pse, mu_slope)
    
    # ---- sample subject-level parameters ----
    theta <- MASS::mvrnorm(n_per_group, mu = mu_vec, Sigma = Sigma)
    colnames(theta) <- c("pse", "slope")
    
    for (s in 1:n_per_group) {
      
      d_i <- design_dd[sample(nrow(design_dd)), ]
      
      diff <- d_i$delayed - d_i$immediate
      
      # psychometric (logistic) choice rule
      p_delayed <- 1 / (1 + exp(-theta[s, "slope"] *
                                  (diff - theta[s, "pse"])))
      
      choice <- rbinom(nrow(d_i), size = 1, prob = p_delayed)
      
      dat[[row_i]] <- d_i %>%
        mutate(
          choose_delayed = choice,
          p_delayed = p_delayed,
          subjectID = paste0(g, "_", s),
          group = g,
          pse = theta[s, "pse"],
          slope = theta[s, "slope"]
        )
      
      row_i <- row_i + 1
    }
  }
  
  bind_rows(dat)
}

```


Example usage:

```{r}
d <- simulate_dd(n_per_group = 10)
str(d)
```

We can visualise the simulated data:

```{r}
#| fig.align: 'center'
#| fig.height: 10
#| fig.width: 10

d %>%
  mutate(diff= delayed-immediate) %>%
  group_by(diff,subjectID, group) %>%
  summarise(choose_delayed = mean(choose_delayed),
            se = mlisi::binomSEM(choose_delayed)) %>%
  ggplot(aes(x=diff, y=choose_delayed, 
             ymin=choose_delayed-se, 
             ymax=choose_delayed+se, 
             color=group)) +
  geom_errorbar(width=0)+
  facet_wrap(.~subjectID, ncol=5)+
  geom_point()
```


### Code: fit model and extract statistics

We can wrap the model-fitting code in a custom function for convenience

```{r}
fit_dd_glmer <- function(dat) {
  
  # --- model: interaction + random slope ---
  dat$diff <- dat$delayed - dat$immediate
  dat$group <- ifelse(dat$group=="PD",1,0)
  fit <-  lme4::glmer(choose_delayed ~ group * diff + (1 + diff | subjectID),
                      data = dat,
                      family = binomial(link = "logit"),
                      control = lme4::glmerControl(optimizer = "bobyqa"))
  
  
  # --- extract Wald tests from model summary (simple & standard) ---
  coefs <- summary(fit)$coefficients
  out <- coefs[rownames(coefs)=="group",]
  names(out) <- c("estimate", "se", "z", "p")
  
  # add other info
  out <- as.data.frame(t(out), stringsAsFactors = FALSE)
  out$N <- length(unique(dat$subjectID))
  
  # add effect on the PSE scale
  slope_CL <- coefs[rownames(coefs)=="diff",1]
  slope_PD <- coefs[rownames(coefs)=="diff",1]
  PSE_CL <- - coefs[rownames(coefs)=="(Intercept)",1] / slope_CL
  PSE_PD <- - (coefs[rownames(coefs)=="(Intercept)",1]+coefs[rownames(coefs)=="group",1]) / slope_PD
  out$delta_PSE <- PSE_PD - PSE_CL
  
  return(out)
}
```

Example usage:

```{r}
fit_dd_glmer(simulate_dd(n_per_group = 20))
```


### Code: power simulation

We now have all the pieces that we need to estimate power by iteratively simulating the experiment with varying sample size (or effect sizes, if necessary). 

```{r, eval=FALSE}
n_sim <- 100
n_participants <- c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 130, 140, 150)

# this bit if you want a progress bar
total_runs <- length(n_participants) * n_sim
pb <- txtProgressBar(min = 0, max = total_runs, style = 3)
run_counter <- 0

# iterate over sample sizes
sim_res <- {}
for(n in n_participants){
  for(i in 1:n_sim){
    sim_i <- sim_fit_dd_glmer(simulate_dd(n_per_group = n)) 
    
    # store output
    sim_i$n_per_group <- n
    sim_i$sim <- i
    sim_res <- rbind(sim_res, sim_i)
  }
}

```

<!-- ADD here code for plotting results -->

## Activity 2 — Experience sampling study

In this activity, we simulate data from an experience sampling (or daily diary) study in which the same participants are measured repeatedly over several days. We then use a multilevel (mixed-effects) model to analyse the data and estimate statistical power for key effects of interest. We can set up our simulations to account for missing observations, or autocorrelation in the predictor.

### Between-person and within-person effects

A central feature of experience sampling data is that observations are **nested within people**. This means that a predictor can vary in two distinct ways:

- **Between people**: some participants tend to score higher than others *on average*.
- **Within a person**: a given participant fluctuates around their own average from day to day.

These two sources of variation often have different psychological meanings and should be analysed separately.

To do this, we decompose the predictor $X$ into two components:

- **Between-person component** (`x_bar`):  
  each participant’s average level of $X$ across days.
- **Within-person component** (`x_cwc`):  
  the day-to-day deviation from that participant’s own mean  
  (often called *cluster-mean centering* or *person-mean centering*).

In the multilevel model, this leads to two separate effects:

- The **between-person effect** (`beta_b`):  
  Do people who are higher on average on $X$ also tend to have higher values of the outcome $Y$?
- The **within-person effect** (`beta_w`):  
  On days when a person is higher than *their own* average on $X$, do they also tend to show higher values of $Y$?

Importantly, these effects need not be the same in size, or even in direction. Treating them separately avoids conflating *differences between people* with *changes within a person over time*.

In this simulation, both components are standardised separately. This means that effect sizes such as `beta_w = 0.20` should be interpreted as:
> a 0.20 SD change in the outcome for a 1 SD change in the corresponding component of the predictor.

This is convenient for setting plausible effect sizes based on common “small / medium / large” conventions.

---

### Temporal dependence and the AR(1) process

In many experience sampling studies, measurements taken close together in time are more similar than measurements taken further apart. For example, how someone feels today is likely to be related to how they felt yesterday. This is known as **temporal autocorrelation**.

In this activity, we model temporal dependence in the *within-person deviations of the predictor* using an **AR(1) process** (autoregressive process of order 1).

#### The AR(1) idea

An AR(1) process assumes that each value depends partly on the previous value plus some new random fluctuation:

$$ \text{out}_t = r \cdot \text{out}_{t-1} + \text{innovation}_t $$

where:
- $r$ is the autocorrelation parameter (between −1 and 1),
- $\text{innovation}_t$ is new random noise at time $t$.

If $r = 0$, observations are independent over time.  
If $r > 0$, consecutive observations tend to be similar ("sticky" over time).

#### Why the variance matters

We want the simulated time series to have a **stable (stationary) variance** over time. For a stationary AR(1) process, the variance of the series satisfies:

$$ \mathrm{Var}(\text{out}) = r^2 \, \mathrm{Var}(\text{out}) + \mathrm{Var}(\text{innovation}) $$

This equation reflects the fact that:
- part of the variance comes from carrying over the previous value ($r^2 \, \mathrm{Var}(\text{out})$),
- part comes from new random input ($\mathrm{Var}(\text{innovation})$).

Solving for the variance of the process gives:

$$ \mathrm{Var}(\text{out}) = \frac{\mathrm{Var}(\text{innovation})}{1 - r^2} $$

This shows that stronger autocorrelation (larger $r$) increases the overall variability of the time series for a given amount of noise.

#### Choosing the innovation variance

In the simulation, we want the AR(1) process to have a **target standard deviation** $s$ (e.g., the within-person SD of the predictor). To achieve this, we set the innovation variance so that:

$$ \mathrm{Var}(\text{out}) = s^2 $$

Substituting into the previous equation:

$$ s^2 = \frac{\mathrm{Var}(\text{innovation})}{1 - r^2} $$

which implies:

$$ \mathrm{SD}(\text{innovation}) = s \sqrt{1 - r^2} $$

This ensures that, regardless of the value of $r$, the simulated within-person deviations have the desired overall variability.

Note that here we include autocorrelation in the predictor but assume that residual errors in the outcome are independent. This keeps the fitted multilevel model correctly specified, while still allowing temporal structure in the data-generating process. It is possible to relax this assumption, but then the model (as it is formulated now, using the `lme4` library) would be miss-specified. This can be OK (perhaps we want to check the effect of model misspecification on power or on false-positive rate). To fit a model that explicitly allows for residual autocorrelation we need a different package (see `?corAR1` in the `nlme` package).

### Code: data simulation

For this example we need these packages:

```{r}
library(lme4)
library(lmerTest)
library(tidyverse)
library(MASS)   # for mvrnorm()
```

We first define a custom function for the AR(1) process, to simulate a vector os size `n` with a set variance `s` and autocorrelation `r`.

```{r}
r_ar1 <- function(n, s, r){
  innov_sd <- s * sqrt(1 - r^2)
  innov <- rnorm(n, mean = 0, sd = innov_sd)
  
  out <- numeric(n)
  out[1] <- rnorm(1, mean = 0, sd = s)
  for (t in 2:n) {
    out[t] <- r * out[t - 1] + innov[t]
  }
  out
}
```


The actual data-simulation function:

```{r}
sim_diary <- function(
    n_child = 50,
    n_days  = 7,
    
    # Predictor X structure
    sd_between_x = 0.6,  # SD of each child's mean X across children (between-person variability)
    sd_within_x  = 0.8,  # SD of day-to-day deviations within a child (within-person variability)
    rho_x_ar1    = 0.2,  # AR(1) correlation in within-person deviations of X across consecutive days
    
    # Outcome Y structure
    beta0   = 0,         # intercept
    beta_w  = 0.20,      # within-person effect of x_cwc on Y  (in SD units of x_cwc)
    beta_b  = 0.20,      # between-person effect of x_bar on Y (in SD units of x_bar)
    tau0    = 0.60,      # SD of random intercepts
    tau1    = 0.15,      # SD of random slopes for x_cwc
    rho_u   = 0.0,       # correlation between intercept and slope random effects
    sigma_e = 0.80,      # residual SD (day-level noise), assumed independent over days
    
    weekday_fx = FALSE,  # include weekday fixed effects in the fitted model?
    
    # Missingness (day-level MCAR)
    mcar_rate = 0.05,    # probability any given day is missing (independent of everything)
    
    seed = NULL
){
  if (!is.null(seed)) set.seed(seed)
  
  # --- Random effects covariance matrix for (u0, u1) ---
  Sigma_u <- matrix(
    c(tau0^2,              rho_u * tau0 * tau1,
      rho_u * tau0 * tau1,        tau1^2),
    nrow = 2, byrow = TRUE
  )
  
  # --- Balanced panel: each child measured each day ---
  df <- expand.grid(id = 1:n_child, day = 1:n_days) %>%
    as_tibble() %>%
    arrange(id, day) %>%
    mutate(
      weekday = factor((day - 1) %% 7,
                       labels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun"))
    )
  
  # --- Person-level mean of X (between-person differences) ---
  x_bar <- rnorm(n_child, mean = 0, sd = sd_between_x)
  df <- df %>%
    left_join(tibble(id = 1:n_child, x_bar = x_bar), by = "id")
  
  # --- Within-person daily deviations of X with AR(1) structure ---
  df <- df %>%
    group_by(id) %>%
    mutate(x_within = r_ar1(n_days, sd_within_x, rho_x_ar1)) %>%
    ungroup() %>%
    mutate(
      x     = x_bar + x_within,
      x_cwc = x - x_bar
    )
  
  # --- Standardise predictors separately ---
  # IMPORTANT: beta_w and beta_b are interpreted in SD units of each component (x_cwc and x_bar).
  df <- df %>%
    mutate(
      x     = as.numeric(scale(x)),
      x_bar = as.numeric(scale(x_bar)),
      x_cwc = as.numeric(scale(x_cwc))
    )
  
  # --- Random effects per child (random intercept + random slope for x_cwc) ---
  RE <- MASS::mvrnorm(n_child, mu = c(0, 0), Sigma = Sigma_u)
  RE <- tibble(id = 1:n_child, u0 = RE[,1], u1 = RE[,2])
  df <- df %>% left_join(RE, by = "id")
  
  # --- Day-level residuals e (independent across days) ---
  # This keeps the fitted lmer() model correctly specified w.r.t. residual correlation.
  df <- df %>%
    mutate(e = rnorm(nrow(df), mean = 0, sd = sigma_e))
  
  # --- True weekday effect (set to zero here; weekday_fx just changes the fitted model) ---
  df <- df %>% mutate(weekday_eff = 0)
  
  # --- Outcome generation ---
  df <- df %>%
    mutate(
      y = beta0 + u0 +
        (beta_w + u1) * x_cwc +
        beta_b * x_bar +
        weekday_eff +
        e
    )
  
  # Optional: standardise outcome
  df <- df %>% mutate(y = as.numeric(scale(y)))
  
  # --- Missingness: day-level MCAR only ---
  if (mcar_rate > 0) {
    keep <- runif(nrow(df)) >= mcar_rate
    df <- df[keep, ]
  }
  
  df
}
```


### Code: fit model and extract statistics

It is convenient to wrap the code for fitting the model and extracting the statistic of interest in a custom function.

```{r}
fit_once <- function(df, weekday_fx = TRUE) {
  
  if (weekday_fx) {
    f <- y ~ x_cwc + x_bar + weekday + (1 + x_cwc | id)
  } else {
    f <- y ~ x_cwc + x_bar + (1 + x_cwc | id)
  }
  
  m <- suppressMessages(lmer(f, data = df, REML = FALSE))
  
  broom.mixed::tidy(m, effects = "fixed", conf.int = TRUE) %>%
    filter(term %in% c("x_cwc", "x_bar")) %>%
    transmute(
      term,
      est = estimate,
      lwr = conf.low,
      upr = conf.high,
      p   = p.value
    )
}
```

### Code: power simulation

We can finally put it all together in a function that iteratively simulate a number of datasets, fit the model on each simulated dataset, and then compute the power as the fraction of significant simulations

```{r}
power_grid <- function( n_sims = 100, weekday_fx = FALSE, ...) {
  all_sims <- vector("list", n_sims)
  
  for (s in 1:n_sims) {
    df  <- sim_diary(seed = s, weekday_fx = weekday_fx, ...)
    est <- fit_once(df, weekday_fx = weekday_fx)
    est$sim <- s
    all_sims[[s]] <- est
  }
  
  res <- bind_rows(all_sims)
  
  res %>%
    group_by(term) %>%
    summarise(
      power_0.05 = mean(p < 0.05, na.rm = TRUE),
      median_est = median(est, na.rm = TRUE),
      mean_est   = mean(est, na.rm = TRUE),
      SE_est     = sd(est, na.rm = TRUE),
      q25        = quantile(est, 0.25, na.rm = TRUE),
      q75        = quantile(est, 0.75, na.rm = TRUE),
      .groups    = "drop"
    )
}
```

Example usage:

```{r, eval=TRUE}

quick_A <- power_grid(
  n_sims = 200,
  n_child = 50,
  n_days = 7,
  beta_w = 0.20,
  beta_b = 0.20,
  tau0 = 0.60,
  tau1 = 0.15,
  sigma_e = 0.80,
  rho_x_ar1 = 0.2,
  mcar_rate = 0.05
)

quick_A

```

